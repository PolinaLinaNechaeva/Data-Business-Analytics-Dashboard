{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNg2i+T+H60HyZNnoiQzZo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PolinaLinaNechaeva/Data-Business-Analytics-Dashboard/blob/main/Analyst_job.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzbmNcqUA3sX",
        "outputId": "b7b7a35b-48b3-434b-bee7-6f2b47597bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows loaded: 932\n",
            "                                          title  \\\n",
            "0                 Business Intelligence Analyst   \n",
            "1  Senior/Staff Data Analyst - People Analytics   \n",
            "2                          Data Analyst (m/w/d)   \n",
            "3                         Business Data Analyst   \n",
            "4                                  Data Analyst   \n",
            "\n",
            "                                 location   postedTime publishedAt  \\\n",
            "0                                 Germany  2 weeks ago  2025-10-30   \n",
            "1                 Berlin, Berlin, Germany   6 days ago  2025-11-13   \n",
            "2  Frankfurt Rhine-Main Metropolitan Area   1 week ago  2025-11-12   \n",
            "3         North Rhine-Westphalia, Germany   1 week ago  2025-11-12   \n",
            "4   Stuttgart, Baden-Württemberg, Germany  3 weeks ago  2025-10-27   \n",
            "\n",
            "                                              jobUrl    companyName  \\\n",
            "0  https://de.linkedin.com/jobs/view/business-int...          Codex   \n",
            "1  https://de.linkedin.com/jobs/view/senior-staff...  Delivery Hero   \n",
            "2  https://de.linkedin.com/jobs/view/data-analyst...        SymBio.   \n",
            "3  https://de.linkedin.com/jobs/view/business-dat...      RJC Group   \n",
            "4  https://de.linkedin.com/jobs/view/data-analyst...      Annapurna   \n",
            "\n",
            "                                          companyUrl  \\\n",
            "0  https://uk.linkedin.com/company/codex-recruitm...   \n",
            "1  https://de.linkedin.com/company/delivery-hero-...   \n",
            "2  https://de.linkedin.com/company/symbio-recruit...   \n",
            "3  https://uk.linkedin.com/company/rjc-group-ltd?...   \n",
            "4  https://uk.linkedin.com/company/annapurna-recr...   \n",
            "\n",
            "                                         description  \\\n",
            "0  Business Intelligence Analyst – Logistikbranch...   \n",
            "1  As the world’s pioneering local delivery platf...   \n",
            "2  SymBio Recruitment GmbH steht für mehr als nur...   \n",
            "3  Senior Business Analyst - Energy Trading - Ger...   \n",
            "4  Freelance Data Analyst\\n\\n\\n\\nLocation: Fully ...   \n",
            "\n",
            "                  applicationsCount contractType  ... companyId  \\\n",
            "0               Over 200 applicants    Full-time  ...   3520744   \n",
            "1  Be among the first 25 applicants    Full-time  ...   2393200   \n",
            "2               Over 200 applicants    Full-time  ...  91654726   \n",
            "3               Over 200 applicants    Full-time  ...  10333877   \n",
            "4               Over 200 applicants    Full-time  ...    735906   \n",
            "\n",
            "                                            applyUrl   applyType benefits  \\\n",
            "0  https://de.linkedin.com/jobs/view/business-int...  EASY_APPLY            \n",
            "1  https://de.linkedin.com/jobs/view/senior-staff...  EASY_APPLY            \n",
            "2  https://de.linkedin.com/jobs/view/data-analyst...  EASY_APPLY            \n",
            "3  https://de.linkedin.com/jobs/view/business-dat...  EASY_APPLY            \n",
            "4  https://de.linkedin.com/jobs/view/data-analyst...  EASY_APPLY            \n",
            "\n",
            "                                      job_title   company_name  \\\n",
            "0                 Business Intelligence Analyst          Codex   \n",
            "1  Senior/Staff Data Analyst - People Analytics  Delivery Hero   \n",
            "2                          Data Analyst (m/w/d)        SymBio.   \n",
            "3                         Business Data Analyst      RJC Group   \n",
            "4                                  Data Analyst      Annapurna   \n",
            "\n",
            "                                             job_url date_posted  \\\n",
            "0  https://de.linkedin.com/jobs/view/business-int...        None   \n",
            "1  https://de.linkedin.com/jobs/view/senior-staff...        None   \n",
            "2  https://de.linkedin.com/jobs/view/data-analyst...        None   \n",
            "3  https://de.linkedin.com/jobs/view/business-dat...        None   \n",
            "4  https://de.linkedin.com/jobs/view/data-analyst...        None   \n",
            "\n",
            "                                   description_clean    source  \n",
            "0  Business Intelligence Analyst – Logistikbranch...  LinkedIn  \n",
            "1  As the world’s pioneering local delivery platf...  LinkedIn  \n",
            "2  SymBio Recruitment GmbH steht für mehr als nur...  LinkedIn  \n",
            "3  Senior Business Analyst - Energy Trading - Ger...  LinkedIn  \n",
            "4  Freelance Data Analyst\\n\\n\\n\\nLocation: Fully ...  LinkedIn  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 932 entries, 0 to 931\n",
            "Data columns (total 26 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   title              932 non-null    object\n",
            " 1   location           932 non-null    object\n",
            " 2   postedTime         932 non-null    object\n",
            " 3   publishedAt        932 non-null    object\n",
            " 4   jobUrl             932 non-null    object\n",
            " 5   companyName        932 non-null    object\n",
            " 6   companyUrl         932 non-null    object\n",
            " 7   description        932 non-null    object\n",
            " 8   applicationsCount  932 non-null    object\n",
            " 9   contractType       932 non-null    object\n",
            " 10  experienceLevel    932 non-null    object\n",
            " 11  workType           932 non-null    object\n",
            " 12  sector             932 non-null    object\n",
            " 13  salary             932 non-null    object\n",
            " 14  posterFullName     932 non-null    object\n",
            " 15  posterProfileUrl   932 non-null    object\n",
            " 16  companyId          932 non-null    object\n",
            " 17  applyUrl           932 non-null    object\n",
            " 18  applyType          932 non-null    object\n",
            " 19  benefits           932 non-null    object\n",
            " 20  job_title          932 non-null    object\n",
            " 21  company_name       932 non-null    object\n",
            " 22  job_url            932 non-null    object\n",
            " 23  date_posted        0 non-null      object\n",
            " 24  description_clean  932 non-null    object\n",
            " 25  source             932 non-null    object\n",
            "dtypes: object(26)\n",
            "memory usage: 189.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset from LinkedIn scraper (JSON file)\n",
        "# Replace with your exact path if different\n",
        "file_path = \"dataset_linkedin-jobs-scraper_2025-11-19_12-03-47-361.json\"\n",
        "\n",
        "# Read JSON\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert JSON list into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# --- Add helper columns ---\n",
        "# Extract job title\n",
        "df[\"job_title\"] = df[\"title\"]\n",
        "\n",
        "# Extract company name (depends on scraper structure)\n",
        "df[\"company_name\"] = df.get(\"companyName\")\n",
        "\n",
        "# Extract job URL\n",
        "df[\"job_url\"] = df.get(\"jobUrl\")\n",
        "\n",
        "# Extract location\n",
        "df[\"location\"] = df.get(\"location\")\n",
        "\n",
        "# Extract date posted\n",
        "df[\"date_posted\"] = pd.to_datetime(df.get(\"listedAt\"), errors=\"coerce\")\n",
        "\n",
        "# Extract description (fallback if descriptionText or descriptionHtml exist)\n",
        "if \"description\" in df.columns:\n",
        "    df[\"description_clean\"] = df[\"description\"]\n",
        "elif \"descriptionText\" in df.columns:\n",
        "    df[\"description_clean\"] = df[\"descriptionText\"]\n",
        "elif \"descriptionHtml\" in df.columns:\n",
        "    df[\"description_clean\"] = df[\"descriptionHtml\"]\n",
        "else:\n",
        "    df[\"description_clean\"] = None\n",
        "\n",
        "# Add job source (useful if combining sources later)\n",
        "df[\"source\"] = \"LinkedIn\"\n",
        "\n",
        "print(\"Rows loaded:\", len(df))\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Save cleaned version\n",
        "df.to_csv(\"linkedin_jobs_clean_2025-11-19.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 0) Load unified cleaned LinkedIn file\n",
        "df = pd.read_csv(\"linkedin_jobs_clean_2025-11-19.csv\")\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 1) Base variables\n",
        "# ----------------------------------------------------------\n",
        "# English-language job titles, simple and clean.\n",
        "# We ignore gender and German inflections here.\n",
        "\n",
        "# Exact roles we want:\n",
        "#   - Data Analyst\n",
        "#   - Business Analyst\n",
        "#   - Data Analytics roles\n",
        "#   - BI Analyst (optional)\n",
        "#   - Data Insights Analyst / Reporting Analyst (optional)\n",
        "\n",
        "ANALYST = r\"(analyst|analytics)\"\n",
        "\n",
        "DATA = r\"(data)\"\n",
        "BUSINESS = r\"(business)\"\n",
        "BI = r\"(bi|business intelligence)\"\n",
        "INSIGHTS = r\"(insights?|reporting)\"\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 2) Patterns for role classification\n",
        "# ----------------------------------------------------------\n",
        "# Each pattern is a named group → same logic as your old code.\n",
        "grp_patterns = {\n",
        "    \"data_analyst\":        rf\"{DATA}\\s*{ANALYST}\",\n",
        "    \"business_analyst\":    rf\"{BUSINESS}\\s*{ANALYST}\",\n",
        "    \"bi_analyst\":          rf\"{BI}\\s*{ANALYST}\",\n",
        "    \"data_reporting\":      rf\"{DATA}.*({INSIGHTS}|reporting)\",\n",
        "    \"analytics_role\":      rf\"{ANALYST}\",   # fallback for generic analyst roles\n",
        "}\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 3) Build combined regex with named groups\n",
        "# ----------------------------------------------------------\n",
        "named_parts = [f\"(?P<{k}>{v})\" for k, v in grp_patterns.items()]\n",
        "pattern = re.compile(\"|\".join(named_parts), flags=re.IGNORECASE)\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 4) Function to detect matched group\n",
        "# ----------------------------------------------------------\n",
        "def match_group(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return None\n",
        "    m = pattern.search(text)\n",
        "    return m.lastgroup if m else None\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 5) Apply classification on job titles\n",
        "# ----------------------------------------------------------\n",
        "search_series = df[\"job_title\"].astype(str)\n",
        "hit_group = search_series.apply(match_group)\n",
        "\n",
        "df_filtered = df.loc[hit_group.notna()].copy()\n",
        "df_filtered[\"job_match_group\"] = hit_group.loc[df_filtered.index].values\n",
        "\n",
        "print(\"Total rows:\", len(df))\n",
        "print(\"Matched:\", len(df_filtered))\n",
        "print(df_filtered[\"job_match_group\"].value_counts())\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 6) Remove duplicates by job URL\n",
        "# ----------------------------------------------------------\n",
        "for col in [\"job_url\", \"url\", \"jobUrl\", \"JobURL\", \"Job URL\"]:\n",
        "    if col in df_filtered.columns:\n",
        "        before = len(df_filtered)\n",
        "        df_filtered = df_filtered.drop_duplicates(subset=[col])\n",
        "        print(f\"Duplicates removed by {col}: {before - len(df_filtered)}\")\n",
        "        break\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 7) Preview final result\n",
        "# ----------------------------------------------------------\n",
        "cols_preview = [\n",
        "    c for c in [\"job_match_group\",\"job_title\",\"title\",\"description_clean\",\"location\"]\n",
        "    if c in df_filtered.columns\n",
        "]\n",
        "print(df_filtered[cols_preview].head(10))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "# 8) Save result\n",
        "# ----------------------------------------------------------\n",
        "df_filtered.to_excel(\"linkedin_jobs_filtered_2025-11-19.xlsx\", index=False)\n",
        "\n",
        "print(f\"Final number of rows: {len(df_filtered)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rS1ft2D-CdGJ",
        "outputId": "a3de5d78-c607-4d51-d052-1e4101082f5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total rows: 932\n",
            "Matched: 485\n",
            "job_match_group\n",
            "data_analyst        245\n",
            "analytics_role      181\n",
            "business_analyst     34\n",
            "bi_analyst           16\n",
            "data_reporting        9\n",
            "Name: count, dtype: int64\n",
            "Duplicates removed by job_url: 0\n",
            "  job_match_group                                     job_title  \\\n",
            "0      bi_analyst                 Business Intelligence Analyst   \n",
            "1    data_analyst  Senior/Staff Data Analyst - People Analytics   \n",
            "2    data_analyst                          Data Analyst (m/w/d)   \n",
            "3    data_analyst                         Business Data Analyst   \n",
            "4    data_analyst                                  Data Analyst   \n",
            "5    data_analyst                     Data Analyst (all gender)   \n",
            "6    data_analyst                Sales Data Analyst (Part-Time)   \n",
            "7  analytics_role         Internship - Data & Analytics (m/f/x)   \n",
            "8    data_analyst                           Data Analyst - D365   \n",
            "9    data_analyst                          Data Analyst (f/m/d)   \n",
            "\n",
            "                                          title  \\\n",
            "0                 Business Intelligence Analyst   \n",
            "1  Senior/Staff Data Analyst - People Analytics   \n",
            "2                          Data Analyst (m/w/d)   \n",
            "3                         Business Data Analyst   \n",
            "4                                  Data Analyst   \n",
            "5                     Data Analyst (all gender)   \n",
            "6                Sales Data Analyst (Part-Time)   \n",
            "7         Internship - Data & Analytics (m/f/x)   \n",
            "8                           Data Analyst - D365   \n",
            "9                          Data Analyst (f/m/d)   \n",
            "\n",
            "                                   description_clean  \\\n",
            "0  Business Intelligence Analyst – Logistikbranch...   \n",
            "1  As the world’s pioneering local delivery platf...   \n",
            "2  SymBio Recruitment GmbH steht für mehr als nur...   \n",
            "3  Senior Business Analyst - Energy Trading - Ger...   \n",
            "4  Freelance Data Analyst\\n\\n\\n\\nLocation: Fully ...   \n",
            "5  Die ALTEN Consulting Services GmbH bietet mit ...   \n",
            "6  Lovehoney Group is the world’s leading sexual ...   \n",
            "7  About FINN\\n\\nFINN is an independent platform ...   \n",
            "8  Data Analyst - Dynamics 365\\nLocation: South-W...   \n",
            "9  WHAT WE'RE LOOKING FOR:\\n\\nWe are seeking a se...   \n",
            "\n",
            "                                 location  \n",
            "0                                 Germany  \n",
            "1                 Berlin, Berlin, Germany  \n",
            "2  Frankfurt Rhine-Main Metropolitan Area  \n",
            "3         North Rhine-Westphalia, Germany  \n",
            "4   Stuttgart, Baden-Württemberg, Germany  \n",
            "5              Augsburg, Bavaria, Germany  \n",
            "6                 Berlin, Berlin, Germany  \n",
            "7                Munich, Bavaria, Germany  \n",
            "8           Lähden, Lower Saxony, Germany  \n",
            "9                 Berlin, Berlin, Germany  \n",
            "Final number of rows: 485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Make a working copy (cleaning happens here)\n",
        "dfc = df_filtered.copy()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 1) Normalize spaces ONLY in string/object columns\n",
        "#    - remove invisible characters\n",
        "#    - collapse multiple spaces\n",
        "#    - strip leading/trailing spaces\n",
        "# -------------------------------------------------------------\n",
        "for col in dfc.columns:\n",
        "    if dfc[col].dtype == object:\n",
        "        dfc[col] = dfc[col].apply(\n",
        "            lambda x: re.sub(r\"\\s+\", \" \", x).strip() if isinstance(x, str) else x\n",
        "        )\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# 2) Remove clearly empty / useless columns\n",
        "#    Some LinkedIn scrapers include optional fields (benefits, applyUrl, companyLogo)\n",
        "#    If a column contains only null/empty values → we drop it\n",
        "# -------------------------------------------------------------\n",
        "drop_cols = []\n",
        "\n",
        "# Drop \"benefits\" column if it is completely empty\n",
        "if \"benefits\" in dfc.columns and dfc[\"benefits\"].nunique(dropna=True) == 0:\n",
        "    drop_cols.append(\"benefits\")\n",
        "\n",
        "# Drop empty \"companyLogo\" if scraper included it\n",
        "if \"companyLogo\" in dfc.columns and dfc[\"companyLogo\"].nunique(dropna=True) == 0:\n",
        "    drop_cols.append(\"companyLogo\")\n",
        "\n",
        "# Drop empty \"applyUrl\" if scraper included it\n",
        "if \"applyUrl\" in dfc.columns and dfc[\"applyUrl\"].nunique(dropna=True) == 0:\n",
        "    drop_cols.append(\"applyUrl\")\n",
        "\n",
        "# Apply the drop\n",
        "dfc = dfc.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "print(\"Dropped columns:\", drop_cols)\n",
        "print(\"Resulting shape:\", dfc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g3OgZkIC4VV",
        "outputId": "806dd35d-1487-4212-d1c9-9836fab828a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped columns: ['benefits']\n",
            "Resulting shape: (485, 26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Regular expressions for detecting seniority level\n",
        "# Adapted for LinkedIn Data/BI Analyst roles\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# Senior-level patterns\n",
        "RE_SENIOR = re.compile(\n",
        "    r'\\b(senior|sr\\.?|sen\\.?|lead|team\\s*lead|principal|head|staff|expert'\n",
        "    r'|leitung|leiter(?:in)?)\\b',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "# Junior-level patterns\n",
        "RE_JUNIOR = re.compile(\n",
        "    r'\\b(junior|jr\\.?|entry|graduate|einsteiger|absolvent|trainee|'\n",
        "    r'werkstudent|working\\s*student|praktikant(?:in)?)\\b',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "# Mid-level patterns\n",
        "RE_MID = re.compile(\n",
        "    r'\\b(mid(?:dle)?|medior|intermediate|professional|experienced|regular)\\b',\n",
        "    re.I\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Function: canonical seniority detection\n",
        "# -------------------------------------------------------------\n",
        "def canon_experience_from_text(text: str) -> str | None:\n",
        "    \"\"\"\n",
        "    Detects seniority level (Junior / Mid / Senior) from text.\n",
        "    Works on LinkedIn titles & descriptions.\n",
        "    Returns:\n",
        "        \"Junior\", \"Mid\", \"Senior\" or None if no clues found.\n",
        "    \"\"\"\n",
        "    if text is None or not isinstance(text, str):\n",
        "        return None\n",
        "\n",
        "    # Normalize text for pattern matching\n",
        "    s = re.sub(r'[/,_-]+', ' ', text.lower()).strip()\n",
        "\n",
        "    # Exclude false positives (e.g., 'Senioren' in German)\n",
        "    s = re.sub(r'\\bsenioren\\b', ' ', s)\n",
        "\n",
        "    # Boolean flags for detected patterns\n",
        "    has_sen = bool(RE_SENIOR.search(s))\n",
        "    has_jun = bool(RE_JUNIOR.search(s))\n",
        "    has_mid = bool(RE_MID.search(s))\n",
        "\n",
        "    # Resolve conflicts or ambiguous cases\n",
        "    if has_sen and not has_jun:\n",
        "        return \"Senior\"\n",
        "    if has_jun and not has_sen:\n",
        "        return \"Junior\"\n",
        "    if has_sen and has_jun:\n",
        "        return \"Mid\"\n",
        "    if has_mid:\n",
        "        return \"Mid\"\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "H8SAo2wvDBpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Cell 4 — Normalize publishedAt + restore from postedTime-like text, then drop postedTime\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "# 0) Fix \"today\" as a constant date (per project conditions)\n",
        "#    For this LinkedIn run, we assume today = 2025-11-19\n",
        "base_date = pd.Timestamp(\"2025-11-19\")  # \"today\" for this dataset\n",
        "yesterday = base_date - pd.Timedelta(days=1)\n",
        "\n",
        "\n",
        "def parse_published_at(s: str):\n",
        "    \"\"\"\n",
        "    Parse publishedAt with different possible formats:\n",
        "    - dd.mm.yyyy\n",
        "    - yyyy-mm-dd or ISO-like with time\n",
        "    - mm/dd/yyyy\n",
        "    Returns pandas.Timestamp or NaT.\n",
        "    \"\"\"\n",
        "    if pd.isna(s):\n",
        "        return pd.NaT\n",
        "    s = str(s).strip()\n",
        "    if not s:\n",
        "        return pd.NaT\n",
        "\n",
        "    # dd.mm.yyyy\n",
        "    if re.fullmatch(r\"\\d{1,2}\\.\\d{1,2}\\.\\d{2,4}\", s):\n",
        "        return pd.to_datetime(s, format=\"%d.%m.%Y\", errors=\"coerce\")\n",
        "\n",
        "    # yyyy-mm-dd (ISO, possibly with time)\n",
        "    if re.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}(T.*)?\", s):\n",
        "        return pd.to_datetime(s, errors=\"coerce\")\n",
        "\n",
        "    # mm/dd/yyyy\n",
        "    if re.fullmatch(r\"\\d{1,2}/\\d{1,2}/\\d{2,4}\", s):\n",
        "        return pd.to_datetime(s, errors=\"coerce\", monthfirst=True)\n",
        "\n",
        "    # Fallback: let pandas try, prefer day-first interpretation\n",
        "    return pd.to_datetime(s, errors=\"coerce\", dayfirst=True)\n",
        "\n",
        "\n",
        "def restore_date_from_posted(txt: str):\n",
        "    \"\"\"\n",
        "    Restore publication date from a relative \"postedTime\"-like text,\n",
        "    for example:\n",
        "      - \"today\", \"yesterday\", \"just now\"\n",
        "      - \"3 days ago\", \"2 weeks ago\", \"1 month ago\"\n",
        "      - \"5 hours ago\" (we treat as 'today')\n",
        "    All dates are calculated relative to base_date.\n",
        "    \"\"\"\n",
        "    if pd.isna(txt):\n",
        "        return pd.NaT\n",
        "    s = str(txt).strip().lower()\n",
        "    if s in {\"\", \"nan\", \"none\", \"null\", \"na\", \"n/a\"}:\n",
        "        return pd.NaT\n",
        "\n",
        "    # today / yesterday / just now\n",
        "    if \"today\" in s or \"just now\" in s:\n",
        "        return base_date\n",
        "    if \"yesterday\" in s:\n",
        "        return yesterday\n",
        "\n",
        "    # months\n",
        "    m = re.search(r\"(\\d+)\\s*month\", s)\n",
        "    if m:\n",
        "        k = int(m.group(1))\n",
        "        return base_date - pd.Timedelta(days=30 * k)\n",
        "\n",
        "    # weeks\n",
        "    m = re.search(r\"(\\d+)\\s*week\", s)\n",
        "    if m:\n",
        "        k = int(m.group(1))\n",
        "        return base_date - pd.Timedelta(days=7 * k)\n",
        "\n",
        "    # days\n",
        "    m = re.search(r\"(\\d+)\\s*day\", s)\n",
        "    if m:\n",
        "        k = int(m.group(1))\n",
        "        return base_date - pd.Timedelta(days=k)\n",
        "\n",
        "    # hours → treat as \"today\"\n",
        "    m = re.search(r\"(\\d+)\\s*hour\", s)\n",
        "    if m:\n",
        "        return base_date\n",
        "\n",
        "    return pd.NaT\n",
        "\n",
        "\n",
        "# 1) Normalize publishedAt → datetime (if column exists)\n",
        "if \"publishedAt\" in dfc.columns:\n",
        "    dfc[\"publishedAt\"] = dfc[\"publishedAt\"].apply(parse_published_at)\n",
        "\n",
        "# 2) Restore missing publishedAt from postedTime-like column (if present)\n",
        "#    For LinkedIn, this might be something like \"postedTime\", \"timeAgo\", etc.\n",
        "if \"postedTime\" in dfc.columns:\n",
        "    mask_missing = dfc[\"publishedAt\"].isna()\n",
        "    if mask_missing.any():\n",
        "        dfc.loc[mask_missing, \"publishedAt\"] = (\n",
        "            dfc.loc[mask_missing, \"postedTime\"].apply(restore_date_from_posted)\n",
        "        )\n",
        "\n",
        "# 3) Drop postedTime helper column if it exists\n",
        "if \"postedTime\" in dfc.columns:\n",
        "    dfc = dfc.drop(columns=[\"postedTime\"])\n",
        "\n",
        "# 4) Add string representation of publishedAt for export (keep datetime as well)\n",
        "dfc[\"publishedAt_str\"] = dfc[\"publishedAt\"].dt.strftime(\"%d.%m.%Y\").fillna(\"\")"
      ],
      "metadata": {
        "id": "35E8BLYiHSPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Helpers for salary parsing\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def _extract_multiplier(s: str) -> int:\n",
        "    \"\"\"\n",
        "    Detects whether the salary is per month or per year based on the text.\n",
        "    Returns an integer multiplier to convert the value to yearly salary.\n",
        "      - monthly → * 12\n",
        "      - yearly → * 1\n",
        "      - if nothing is clear → assume yearly\n",
        "    \"\"\"\n",
        "    s_low = s.lower()\n",
        "\n",
        "    # Monthly payment → multiply by 12\n",
        "    if any(tag in s_low for tag in [\"/mo\", \"/month\", \"per month\", \"monat\"]):\n",
        "        return 12\n",
        "\n",
        "    # Yearly payment → multiplier 1\n",
        "    if any(tag in s_low for tag in [\"/yr\", \"/year\", \"per year\", \"p.a.\", \"pa\", \"jahr\"]):\n",
        "        return 1\n",
        "\n",
        "    # No explicit period → assume yearly salary\n",
        "    return 1\n",
        "\n",
        "\n",
        "# Matches thousand separators (1.234 / 1,234 / 1 234)\n",
        "_thousands_sep_re = re.compile(r\"(?<=\\d)[\\.,\\s](?=\\d{3}(\\D|$))\")\n",
        "\n",
        "\n",
        "def _to_number(token: str) -> float | None:\n",
        "    \"\"\"\n",
        "    Convert a token like '60k', '75.000', '100 000', '90,5' into a float.\n",
        "    Handles:\n",
        "      - 'k' and 'm' suffixes\n",
        "      - different thousand separators\n",
        "      - commas vs dots as decimal separator\n",
        "    Returns float or None if cannot parse.\n",
        "    \"\"\"\n",
        "    if not token:\n",
        "        return None\n",
        "\n",
        "    t = token.strip().lower()\n",
        "\n",
        "    # Suffixes: k, m (thousands, millions)\n",
        "    mult_suffix = 1.0\n",
        "    if t.endswith(\"k\"):\n",
        "        mult_suffix = 1_000.0\n",
        "        t = t[:-1]\n",
        "    elif t.endswith(\"m\"):\n",
        "        mult_suffix = 1_000_000.0\n",
        "        t = t[:-1]\n",
        "\n",
        "    # Remove currencies and letters, keep only digits, dots and commas\n",
        "    t = re.sub(r\"[^\\d\\.,]\", \"\", t)\n",
        "\n",
        "    # Remove thousand separators: dot, comma or space before 3 digits\n",
        "    t = _thousands_sep_re.sub(\"\", t)\n",
        "\n",
        "    # Normalize decimal separator: comma → dot\n",
        "    t = t.replace(\",\", \".\")\n",
        "\n",
        "    try:\n",
        "        return float(t) * mult_suffix if t else None\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "\n",
        "def _parse_one_side(s: str, multiplier_hint: int | None = None) -> float | None:\n",
        "    \"\"\"\n",
        "    Parse one side of the salary range and apply period multiplier.\n",
        "    Example: '60k / year' → 60000\n",
        "    \"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "\n",
        "    m = _extract_multiplier(s) if multiplier_hint is None else multiplier_hint\n",
        "    val = _to_number(s)\n",
        "    return (val * m) if val is not None else None\n",
        "\n",
        "\n",
        "def split_salary_cell(cell):\n",
        "    \"\"\"\n",
        "    Split salary cell into min and max annual salary in EUR-like units.\n",
        "    Handles formats like:\n",
        "      - '60k'\n",
        "      - '60k - 80k'\n",
        "      - '60.000 - 80.000 € p.a.'\n",
        "      - '5 000 / month'\n",
        "    Returns a Series with:\n",
        "      - 'salary_min (€)'\n",
        "      - 'salary_max (€)'\n",
        "    \"\"\"\n",
        "    # Empty values\n",
        "    if pd.isna(cell):\n",
        "        return pd.Series({\"salary_min (€)\": np.nan, \"salary_max (€)\": np.nan})\n",
        "\n",
        "    s = str(cell).strip()\n",
        "    if s.lower() in {\"\", \"nan\", \"none\", \"null\", \"na\", \"n/a\"}:\n",
        "        return pd.Series({\"salary_min (€)\": np.nan, \"salary_max (€)\": np.nan})\n",
        "\n",
        "    # Common multiplier (if period is specified only once in the text)\n",
        "    mult = _extract_multiplier(s)\n",
        "\n",
        "    # Split range by hyphen or en dash\n",
        "    parts = re.split(r\"\\s*[-–]\\s*\", s)\n",
        "    parts = [p for p in parts if p]\n",
        "\n",
        "    # Single value → use it as both min and max\n",
        "    if len(parts) == 1:\n",
        "        v = _parse_one_side(parts[0], multiplier_hint=mult)\n",
        "        return pd.Series({\"salary_min (€)\": v, \"salary_max (€)\": v})\n",
        "\n",
        "    # Two values → min and max\n",
        "    v1 = _parse_one_side(parts[0], multiplier_hint=mult)\n",
        "    v2 = _parse_one_side(parts[1], multiplier_hint=mult)\n",
        "    return pd.Series({\"salary_min (€)\": v1, \"salary_max (€)\": v2})\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Apply salary parsing if salary column exists\n",
        "# -------------------------------------------------------------\n",
        "if \"salary\" in dfc.columns:\n",
        "    salary_split = dfc[\"salary\"].apply(split_salary_cell)\n",
        "\n",
        "    # Drop original raw salary text\n",
        "    dfc = dfc.drop(columns=[\"salary\"])\n",
        "\n",
        "    # Attach parsed numeric columns\n",
        "    dfc[[\"salary_min (€)\", \"salary_max (€)\"]] = salary_split"
      ],
      "metadata": {
        "id": "czHqi3NoHb4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# Split 'location' into city / region / country and drop original column\n",
        "# Typical LinkedIn format: \"City, Region, Country\"\n",
        "# Examples:\n",
        "#   \"Berlin, Berlin, Germany\"\n",
        "#   \"Munich, Bavaria, Germany\"\n",
        "#   \"Hamburg, Germany\"\n",
        "# -------------------------------------------------------------\n",
        "if \"location\" in dfc.columns:\n",
        "    # Make sure we work with strings and replace NaN with empty string\n",
        "    loc = dfc[\"location\"].fillna(\"\").astype(str)\n",
        "\n",
        "    # Split into up to 3 parts by comma: p0, p1, p2\n",
        "    parts = loc.str.split(\",\", n=2, expand=True).fillna(\"\")\n",
        "\n",
        "    # Ensure we always have exactly 3 columns (p0, p1, p2)\n",
        "    while parts.shape[1] < 3:\n",
        "        parts[parts.shape[1]] = \"\"\n",
        "\n",
        "    # Strip whitespace in each part\n",
        "    parts = parts.apply(lambda s: s.str.strip())\n",
        "    parts.columns = [\"p0\", \"p1\", \"p2\"]\n",
        "\n",
        "    # Count how many non-empty parts we have for each row\n",
        "    cnt = (parts != \"\").sum(axis=1)\n",
        "\n",
        "    # Initialize city / region / country\n",
        "    city    = parts[\"p0\"]\n",
        "    region  = pd.Series([\"\"] * len(parts), index=parts.index, dtype=object)\n",
        "    country = pd.Series([\"\"] * len(parts), index=parts.index, dtype=object)\n",
        "\n",
        "    # Case 2: exactly two parts → city, country (no explicit region)\n",
        "    mask2 = (cnt == 2)\n",
        "    country.loc[mask2] = parts.loc[mask2, \"p1\"]\n",
        "\n",
        "    # Case 3+: three parts → city, region, country\n",
        "    mask3 = (cnt >= 3)\n",
        "    region.loc[mask3]  = parts.loc[mask3, \"p1\"]\n",
        "    country.loc[mask3] = parts.loc[mask3, \"p2\"]\n",
        "\n",
        "    # Drop original 'location' and add new normalized columns\n",
        "    dfc = dfc.drop(columns=[\"location\"])\n",
        "    dfc[[\"city\", \"region\", \"country\"]] = pd.concat([city, region, country], axis=1)\n",
        "\n",
        "# Quick sanity check\n",
        "try:\n",
        "    display(dfc[[\"city\", \"region\", \"country\"]].head(10))\n",
        "except:\n",
        "    print(dfc[[\"city\", \"region\", \"country\"]].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "id": "XScbVxG5HubQ",
        "outputId": "7cb44dc8-60c8-4d29-9d02-24249127d834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                     city             region  country\n",
              "0                                 Germany                            \n",
              "1                                  Berlin             Berlin  Germany\n",
              "2  Frankfurt Rhine-Main Metropolitan Area                            \n",
              "3                  North Rhine-Westphalia                     Germany\n",
              "4                               Stuttgart  Baden-Württemberg  Germany\n",
              "5                                Augsburg            Bavaria  Germany\n",
              "6                                  Berlin             Berlin  Germany\n",
              "7                                  Munich            Bavaria  Germany\n",
              "8                                  Lähden       Lower Saxony  Germany\n",
              "9                                  Berlin             Berlin  Germany"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9a9dfb7e-07c3-4bef-a67f-52f2c535b25a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>city</th>\n",
              "      <th>region</th>\n",
              "      <th>country</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Germany</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Berlin</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Frankfurt Rhine-Main Metropolitan Area</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>North Rhine-Westphalia</td>\n",
              "      <td></td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Stuttgart</td>\n",
              "      <td>Baden-Württemberg</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Augsburg</td>\n",
              "      <td>Bavaria</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Berlin</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Munich</td>\n",
              "      <td>Bavaria</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Lähden</td>\n",
              "      <td>Lower Saxony</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Berlin</td>\n",
              "      <td>Berlin</td>\n",
              "      <td>Germany</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9a9dfb7e-07c3-4bef-a67f-52f2c535b25a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9a9dfb7e-07c3-4bef-a67f-52f2c535b25a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9a9dfb7e-07c3-4bef-a67f-52f2c535b25a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-84df1870-426d-4548-8997-6ac4f0aa673a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-84df1870-426d-4548-8997-6ac4f0aa673a')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-84df1870-426d-4548-8997-6ac4f0aa673a button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(dfc[[\\\"city\\\", \\\"region\\\", \\\"country\\\"]]\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"city\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"Berlin\",\n          \"Augsburg\",\n          \"Germany\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"region\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Berlin\",\n          \"Lower Saxony\",\n          \"Baden-W\\u00fcrttemberg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Germany\",\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if \"postedTime\" in dfc.columns:\n",
        "    dfc = dfc.drop(columns=[\"postedTime\"])"
      ],
      "metadata": {
        "id": "VvmLs6GdKp0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# General-purpose cleaners for remaining fields\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def clean_str_nan(x):\n",
        "    \"\"\"\n",
        "    Normalize string-like fields:\n",
        "    - convert 'nan', 'none', 'null', 'na', 'n/a' → empty string\n",
        "    - strip whitespace\n",
        "    Keeps numbers untouched.\n",
        "    \"\"\"\n",
        "    if pd.isna(x):\n",
        "        return \"\"\n",
        "    s = str(x).strip()\n",
        "    if s.lower() in [\"nan\", \"none\", \"null\", \"na\", \"n/a\"]:\n",
        "        return \"\"\n",
        "    return s\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Apply basic cleaning to specific LinkedIn columns if present\n",
        "# -------------------------------------------------------------\n",
        "for col in [\"posterProfileUrl\", \"posterFullName\", \"salary\"]:\n",
        "    if col in dfc.columns:\n",
        "        dfc[col] = dfc[col].apply(clean_str_nan)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Clean experienceLevel (LinkedIn sometimes adds \"level\")\n",
        "# Example: \"Mid Level\" → \"Mid\"\n",
        "# -------------------------------------------------------------\n",
        "if \"experienceLevel\" in dfc.columns:\n",
        "    dfc[\"experienceLevel\"] = (\n",
        "        dfc[\"experienceLevel\"]\n",
        "        .apply(clean_str_nan)\n",
        "        .str.replace(\"level\", \"\", case=False, regex=False)\n",
        "        .str.strip()\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Normalize applicationsCount\n",
        "# LinkedIn formats could be:\n",
        "#   \"Be among the first 25 applicants\"\n",
        "#   \"Over 200 applicants\"\n",
        "#   \"30 applicants\"\n",
        "#   \"Über 100 Bewerbungen\"\n",
        "# This function converts them into integers where possible.\n",
        "# -------------------------------------------------------------\n",
        "def clean_applications_count(x):\n",
        "    if pd.isna(x) or str(x).strip() == \"\":\n",
        "        return \"\"\n",
        "\n",
        "    s = str(x).lower()\n",
        "\n",
        "    # \"Be among the first 25 applicants\" → 25\n",
        "    if \"first 25\" in s:\n",
        "        return 25\n",
        "\n",
        "    # \"over 200 applicants\" or \"Über 200\"\n",
        "    match_over = re.search(r\"(over|über)\\s*(\\d+)\", s)\n",
        "    if match_over:\n",
        "        return int(match_over.group(2)) + 1\n",
        "\n",
        "    # general numeric extraction\n",
        "    match = re.search(r\"\\d+\", s.replace(\".\", \"\"))\n",
        "    if match:\n",
        "        return int(match.group())\n",
        "\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "if \"applicationsCount\" in dfc.columns:\n",
        "    dfc[\"applicationsCount\"] = dfc[\"applicationsCount\"].apply(clean_applications_count)\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Final clean-up: replace all 'nan-like' texts across dataframe\n",
        "# -------------------------------------------------------------\n",
        "dfc = dfc.applymap(\n",
        "    lambda x: \"\" if str(x).strip().lower() in [\"nan\", \"none\", \"null\", \"na\", \"n/a\"] else x\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-Ao1Xn-KyFs",
        "outputId": "f81f0934-68e6-47e2-8278-ea6151ef4fed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3329588626.py:80: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  dfc = dfc.applymap(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Extract analytics-related programs/tools from job descriptions and explode ---\n",
        "\n",
        "import re\n",
        "\n",
        "# Patterns for concrete analytics tools (BI, databases, languages, ETL, cloud, etc.)\n",
        "PROGRAM_PATTERNS = [\n",
        "    # BI / Reporting\n",
        "    (r\"\\bpower\\s*bi\\b\", \"Power BI\"),\n",
        "    (r\"\\btableau\\b\", \"Tableau\"),\n",
        "    (r\"\\blooker\\b|\\blooker\\s*studio\\b\", \"Looker\"),\n",
        "    (r\"\\bqlik(?:view|sense)?\\b\", \"Qlik\"),\n",
        "    (r\"\\bpower\\s*query\\b\", \"Power Query\"),\n",
        "    (r\"\\bdax\\b\", \"DAX\"),\n",
        "    (r\"\\bssrs\\b\", \"SSRS\"),\n",
        "\n",
        "    # Spreadsheets\n",
        "    (r\"\\bexcel\\b\", \"Excel\"),\n",
        "    (r\"\\bgoogle\\s*sheets?\\b\", \"Google Sheets\"),\n",
        "\n",
        "    # Databases / SQL engines\n",
        "    (r\"\\bms\\s*sql\\s*server\\b|\\bsql\\s*server\\b\", \"SQL Server\"),\n",
        "    (r\"\\bpostgres(?:ql)?\\b\", \"PostgreSQL\"),\n",
        "    (r\"\\bmysql\\b\", \"MySQL\"),\n",
        "    (r\"\\boracle\\b\", \"Oracle\"),\n",
        "    (r\"\\bsnowflake\\b\", \"Snowflake\"),\n",
        "    (r\"\\bbigquery\\b\", \"BigQuery\"),\n",
        "    (r\"\\bredshift\\b\", \"Redshift\"),\n",
        "    (r\"\\bsqlite\\b\", \"SQLite\"),\n",
        "    (r\"\\bteradata\\b\", \"Teradata\"),\n",
        "    (r\"\\bclickhouse\\b\", \"ClickHouse\"),\n",
        "\n",
        "    # Data platforms / warehouses / lakehouses\n",
        "    (r\"\\bdatabricks\\b\", \"Databricks\"),\n",
        "    (r\"\\bdata\\s*lake\\b\", \"Data Lake\"),\n",
        "    (r\"\\bdata\\s*warehouse\\b\", \"Data Warehouse\"),\n",
        "\n",
        "    # Data integration / ETL / Orchestration\n",
        "    (r\"\\bairflow\\b\", \"Apache Airflow\"),\n",
        "    (r\"\\bfivetran\\b\", \"Fivetran\"),\n",
        "    (r\"\\bmatillion\\b\", \"Matillion\"),\n",
        "    (r\"\\binformatica\\b\", \"Informatica\"),\n",
        "    (r\"\\bssis\\b\", \"SSIS\"),\n",
        "    (r\"\\btalend\\b\", \"Talend\"),\n",
        "    (r\"\\bdbt\\b\", \"dbt\"),\n",
        "\n",
        "    # Programming languages for analytics\n",
        "    (r\"\\bpython\\b\", \"Python\"),\n",
        "    (r\"\\br\\b\", \"R\"),\n",
        "    (r\"\\bscala\\b\", \"Scala\"),\n",
        "    (r\"\\bmatlab\\b\", \"MATLAB\"),\n",
        "    (r\"\\bsas\\b\", \"SAS\"),\n",
        "    (r\"\\bstata\\b\", \"Stata\"),\n",
        "\n",
        "    # Cloud platforms (as tools in the stack)\n",
        "    (r\"\\bazure\\b\", \"Azure\"),\n",
        "    (r\"\\baws\\b\", \"AWS\"),\n",
        "    (r\"\\bamazon\\s*web\\s*services\\b\", \"AWS\"),\n",
        "    (r\"\\bgcp\\b|\\bgoogle\\s*cloud\\b\", \"GCP\"),\n",
        "    (r\"\\bazure\\s*synapse\\b\", \"Azure Synapse\"),\n",
        "    (r\"\\bazure\\s*data\\s*factory\\b\", \"Azure Data Factory\"),\n",
        "\n",
        "    # SAP analytics\n",
        "    (r\"\\bsap\\s*bw\\b\", \"SAP BW\"),\n",
        "    (r\"\\bsap\\s*hana\\b\", \"SAP HANA\"),\n",
        "\n",
        "    # Other analytics-related tools\n",
        "    (r\"\\bpower\\s*pivot\\b\", \"Power Pivot\"),\n",
        "    (r\"\\bknime\\b\", \"KNIME\"),\n",
        "    (r\"\\brapidminer\\b\", \"RapidMiner\"),\n",
        "]\n",
        "\n",
        "\n",
        "def extract_programs_from_text(text: str):\n",
        "    \"\"\"\n",
        "    Extract a list of analytics-related tools/programs from free text.\n",
        "    Returns a list of unique labels in the order they appear.\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    t = text.lower()\n",
        "    found = []\n",
        "\n",
        "    for pat, label in PROGRAM_PATTERNS:\n",
        "        if re.search(pat, t, flags=re.IGNORECASE):\n",
        "            found.append(label)\n",
        "\n",
        "    # Deduplicate while preserving order\n",
        "    seen, uniq = set(), []\n",
        "    for x in found:\n",
        "        if x not in seen:\n",
        "            seen.add(x)\n",
        "            uniq.append(x)\n",
        "    return uniq\n",
        "\n",
        "\n",
        "# 1) Choose source column for text (prefer cleaned description)\n",
        "if \"description_clean\" in dfc.columns:\n",
        "    desc_col = \"description_clean\"\n",
        "elif \"description\" in dfc.columns:\n",
        "    desc_col = \"description\"\n",
        "elif \"descriptions\" in dfc.columns:\n",
        "    desc_col = \"descriptions\"\n",
        "else:\n",
        "    raise KeyError(\"No suitable description column found in dfc \"\n",
        "                   \"(expected 'description_clean', 'description' or 'descriptions').\")\n",
        "\n",
        "# 2) Create a list of programs/tools per row\n",
        "dfc[\"Programs\"] = dfc[desc_col].apply(extract_programs_from_text)\n",
        "\n",
        "# 3) Explode: one row per (job, program)\n",
        "dfc = dfc.explode(\"Programs\", ignore_index=True)\n",
        "\n",
        "# 4) Drop empty / NaN programs\n",
        "mask = dfc[\"Programs\"].notna() & (dfc[\"Programs\"].astype(str).str.strip() != \"\")\n",
        "dfc = dfc[mask].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "GLgfKWo0Libg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install xlsxwriter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-R_RvZ5MBHH",
        "outputId": "f247f720-dc37-4caa-bbd4-40e766190789"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xlsxwriter\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/175.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m174.1/175.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-3.2.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXTRACTION OF SKILLS/REQUIREMENTS AND PRETTY EXPORT FOR ANALYTICS ROLES\n",
        "\n",
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Choose description column (prefer cleaned one if available)\n",
        "if \"description_clean\" in dfc.columns:\n",
        "    DESC_COL = \"description_clean\"\n",
        "elif \"description\" in dfc.columns:\n",
        "    DESC_COL = \"description\"\n",
        "else:\n",
        "    raise KeyError(\"Expected a description column: 'description_clean' or 'description'.\")\n",
        "\n",
        "# -------- dictionaries / patterns tuned for DATA / BI / ANALYTICS ROLES --------\n",
        "\n",
        "HARD_SKILLS = {\n",
        "    \"sql / databases\": [\n",
        "        r\"\\bsql\\b\",\n",
        "        r\"\\b(t[-\\s]?sql|tsql)\\b\",\n",
        "        r\"\\bpl/sql\\b\",\n",
        "        r\"\\b(postgres(?:ql)?|postgres)\\b\",\n",
        "        r\"\\bmysql\\b\",\n",
        "        r\"\\boracle\\b\",\n",
        "        r\"\\bsnowflake\\b\",\n",
        "        r\"\\bbigquery\\b\",\n",
        "        r\"\\bredshift\\b\",\n",
        "        r\"\\bsql\\s*server\\b\",\n",
        "        r\"\\bsqlite\\b\",\n",
        "        r\"\\bnosql\\b\",\n",
        "        r\"\\bmongodb\\b\",\n",
        "        r\"\\bdata\\s*warehouse\\b\",\n",
        "        r\"\\bdata\\s*lake\\b\",\n",
        "    ],\n",
        "    \"programming / scripting\": [\n",
        "        r\"\\bpython\\b\",\n",
        "        r\"\\bpandas\\b\",\n",
        "        r\"\\bnumpy\\b\",\n",
        "        r\"\\bscikit[-\\s]*learn\\b\",\n",
        "        r\"\\b(py)?spark\\b\",\n",
        "        r\"\\bpyspark\\b\",\n",
        "        r\"\\br\\b\",\n",
        "        r\"\\bscala\\b\",\n",
        "        r\"\\bmatlab\\b\",\n",
        "        r\"\\bsas\\b\",\n",
        "        r\"\\bstata\\b\",\n",
        "        r\"\\b(java(script)?|c\\+\\+?|go|rust)\\b\",\n",
        "    ],\n",
        "    \"bi / reporting tools\": [\n",
        "        r\"\\bpower\\s*bi\\b\",\n",
        "        r\"\\btableau\\b\",\n",
        "        r\"\\blooker\\b\",\n",
        "        r\"\\blooker\\s*studio\\b\",\n",
        "        r\"\\bqlik(view|sense)?\\b\",\n",
        "        r\"\\bssrs\\b\",\n",
        "        r\"\\bssas\\b\",\n",
        "        r\"\\b(cognos|microstrategy)\\b\",\n",
        "        r\"\\bdashboards?\\b\",\n",
        "        r\"\\b(kpi|okrs?)\\b\",\n",
        "    ],\n",
        "    \"excel / spreadsheets\": [\n",
        "        r\"\\bexcel\\b\",\n",
        "        r\"\\b(vlookup|index\\s*\\+?\\s*match|pivot\\s*table)s?\\b\",\n",
        "        r\"\\b(spreadsheets?|google\\s*sheets?)\\b\",\n",
        "    ],\n",
        "    \"data modeling / warehousing\": [\n",
        "        r\"\\bdata\\s*model(ling|ing)\\b\",\n",
        "        r\"\\bdimensional\\s*model(ling|ing)\\b\",\n",
        "        r\"\\bstar\\s*schema\\b\",\n",
        "        r\"\\bsnowflake\\s*schema\\b\",\n",
        "        r\"\\b(etl|elt)\\b\",\n",
        "        r\"\\bdata\\s*pipeline(s)?\\b\",\n",
        "        r\"\\bdbt\\b\",\n",
        "    ],\n",
        "    \"etl / orchestration\": [\n",
        "        r\"\\betl\\b\",\n",
        "        r\"\\belt\\b\",\n",
        "        r\"\\bdata\\s*integration\\b\",\n",
        "        r\"\\bdata\\s*ingestion\\b\",\n",
        "        r\"\\bapache\\s*airflow\\b\",\n",
        "        r\"\\bairflow\\b\",\n",
        "        r\"\\b(informatica|talend|matillion|fivetran)\\b\",\n",
        "        r\"\\bss(is|is)\\b\",\n",
        "    ],\n",
        "    \"statistics / maths\": [\n",
        "        r\"\\bstatistics\\b\",\n",
        "        r\"\\bstatistical\\b\",\n",
        "        r\"\\bhypothesis\\s*testing\\b\",\n",
        "        r\"\\b(a/b|ab)\\s*testing\\b\",\n",
        "        r\"\\bregression\\b\",\n",
        "        r\"\\banova\\b\",\n",
        "        r\"\\btime[-\\s]*series\\b\",\n",
        "        r\"\\bprobability\\b\",\n",
        "        r\"\\blinear\\s*algebra\\b\",\n",
        "        r\"\\bmath(ematics)?\\b\",\n",
        "    ],\n",
        "    \"machine learning / data science\": [\n",
        "        r\"\\bmachine\\s*learning\\b\",\n",
        "        r\"\\bml\\b\",\n",
        "        r\"\\bdeep\\s*learning\\b\",\n",
        "        r\"\\b(neural\\s*networks?|nn)\\b\",\n",
        "        r\"\\bclustering\\b\",\n",
        "        r\"\\bclassification\\b\",\n",
        "        r\"\\bforecasting\\b\",\n",
        "        r\"\\brecommendation\\s*systems?\\b\",\n",
        "    ],\n",
        "    \"cloud / big data\": [\n",
        "        r\"\\baws\\b\",\n",
        "        r\"\\bazure\\b\",\n",
        "        r\"\\bgcp\\b\",\n",
        "        r\"\\bgoogle\\s*cloud\\b\",\n",
        "        r\"\\bamazon\\s*web\\s*services\\b\",\n",
        "        r\"\\bazure\\s*synapse\\b\",\n",
        "        r\"\\bazure\\s*data\\s*factory\\b\",\n",
        "        r\"\\bdatabricks\\b\",\n",
        "        r\"\\bhadoop\\b\",\n",
        "        r\"\\bspark\\b\",\n",
        "        r\"\\bkafka\\b\",\n",
        "    ],\n",
        "    \"version control / ci\": [\n",
        "        r\"\\bgit\\b\",\n",
        "        r\"\\bgithub\\b\",\n",
        "        r\"\\bgitlab\\b\",\n",
        "        r\"\\bbitbucket\\b\",\n",
        "        r\"\\bci/?cd\\b\",\n",
        "        r\"\\bjenkins\\b\",\n",
        "        r\"\\bdocker\\b\",\n",
        "    ],\n",
        "    \"business / product analytics\": [\n",
        "        r\"\\bcohort\\s*analysis\\b\",\n",
        "        r\"\\bunit\\s*economics\\b\",\n",
        "        r\"\\bfunnel(s)?\\b\",\n",
        "        r\"\\bretention\\b\",\n",
        "        r\"\\bchurn\\b\",\n",
        "        r\"\\bclick[-\\s]*through\\s*rate\\b\",\n",
        "        r\"\\bconversion\\s*rate\\b\",\n",
        "        r\"\\bproduct\\s*analytics?\\b\",\n",
        "        r\"\\bpricing\\s*analytics?\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "SOFT_SKILLS = {\n",
        "    \"communication / stakeholder\": [\n",
        "        r\"\\bstakeholder(s)?\\b\",\n",
        "        r\"\\bstakeholder\\s*management\\b\",\n",
        "        r\"\\bstakeholder\\s*communication\\b\",\n",
        "        r\"\\bcommunication\\b\",\n",
        "        r\"\\bkommunikation\\b\",\n",
        "        r\"\\bability to explain\\b\",\n",
        "        r\"\\btranslate\\b.*\\bcomplex\\b.*\\binto\\b.*\\bsimple\\b\",\n",
        "        r\"\\bstorytelling\\b\",\n",
        "        r\"\\bdata\\s*storytelling\\b\",\n",
        "        r\"\\bpräsentationsfähigkeit\\b\",\n",
        "    ],\n",
        "    \"problem solving / critical thinking\": [\n",
        "        r\"\\bproblem[-\\s]*solving\\b\",\n",
        "        r\"\\bcritical\\s*thinking\\b\",\n",
        "        r\"\\bstructured\\s*(thinking|approach)\\b\",\n",
        "        r\"\\banalytical\\s*(mindset|skills?)\\b\",\n",
        "        r\"\\bstrong analytical\\b\",\n",
        "        r\"\\broot cause\\b\",\n",
        "    ],\n",
        "    \"ownership / proactivity\": [\n",
        "        r\"\\bownership\\b\",\n",
        "        r\"\\bproactive\\b\",\n",
        "        r\"\\bself[-\\s]*starter\\b\",\n",
        "        r\"\\binitiative\\b\",\n",
        "        r\"\\bhands[-\\s]*on\\b\",\n",
        "        r\"\\bcan[-\\s]*do\\b\",\n",
        "        r\"\\bverantwortungs(bewusstsein)?\\b\",\n",
        "        r\"\\beigenständig\\b\",\n",
        "        r\"\\bselbstständig\\b\",\n",
        "    ],\n",
        "    \"business acumen / product thinking\": [\n",
        "        r\"\\bbusiness\\s*acumen\\b\",\n",
        "        r\"\\bcommercial\\s*mindset\\b\",\n",
        "        r\"\\bunderstanding of business\\b\",\n",
        "        r\"\\bproduct\\s*thinking\\b\",\n",
        "        r\"\\bbusiness\\s*partner\\b\",\n",
        "        r\"\\bbetreibliches\\s*verständnis\\b\",\n",
        "    ],\n",
        "    \"self-organization / prioritization\": [\n",
        "        r\"\\bprioriti[sz]ation\\b\",\n",
        "        r\"\\btime\\s*management\\b\",\n",
        "        r\"\\bself[-\\s]*organized\\b\",\n",
        "        r\"\\bself[-\\s]*organisation\\b\",\n",
        "        r\"\\borganized\\b\",\n",
        "        r\"\\bstrukturierte\\s*arbeitsweise\\b\",\n",
        "    ],\n",
        "    \"teamwork / collaboration\": [\n",
        "        r\"\\bteamwork\\b\",\n",
        "        r\"\\bteam\\s*player\\b\",\n",
        "        r\"\\bcollaborative\\b\",\n",
        "        r\"\\bcross[-\\s]*functional\\b\",\n",
        "    ],\n",
        "    \"attention to detail / quality\": [\n",
        "        r\"\\battention to detail\\b\",\n",
        "        r\"\\bdetail[-\\s]*oriented\\b\",\n",
        "        r\"\\bsorgfalt\\b\",\n",
        "        r\"\\bgenauigkeit\\b\",\n",
        "        r\"\\bquality[-\\s]*driven\\b\",\n",
        "    ],\n",
        "    \"adaptability / learning\": [\n",
        "        r\"\\badaptab(le|ility)\\b\",\n",
        "        r\"\\bfast\\s*learner\\b\",\n",
        "        r\"\\bcurious(ity)?\\b\",\n",
        "        r\"\\bcontinuous\\s*learning\\b\",\n",
        "        r\"\\bopen\\s*to\\s*feedback\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "LANG_PATTERNS = {\n",
        "    \"en\": [\n",
        "        r\"\\benglish\\b\",\n",
        "        r\"\\benglisch\\b\",\n",
        "        r\"\\bfluent in english\\b\",\n",
        "        r\"\\bc1 (english|englisch)\\b\",\n",
        "    ],\n",
        "    \"de\": [\n",
        "        r\"\\bgerman\\b\",\n",
        "        r\"\\bdeutsch\\b\",\n",
        "        r\"\\bfließend(e[rsn]?)?\\s*deutsch\\b\",\n",
        "        r\"\\bc1 (german|deutsch)\\b\",\n",
        "        r\"\\bverhandlungssicher(es)? deutsch\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "DEGREE_MAP = {\n",
        "    \"bachelor\": [\n",
        "        r\"\\bbachelor('?s)?\\b\",\n",
        "        r\"\\bbsc\\b\",\n",
        "        r\"\\bb\\.sc\\.\\b\",\n",
        "        r\"\\bdegree\\b\",\n",
        "    ],\n",
        "    \"master\": [\n",
        "        r\"\\bmaster('?s)?\\b\",\n",
        "        r\"\\bmsc\\b\",\n",
        "        r\"\\bm\\.sc\\.\\b\",\n",
        "    ],\n",
        "    \"data / quant degree\": [\n",
        "        r\"\\b(data\\s*science|statistics?|mathematics?|computer\\s*science)\\b\",\n",
        "        r\"\\b(stem)\\b\",\n",
        "        r\"\\bquant(itative)?\\b\",\n",
        "    ],\n",
        "    \"ausbildung\": [\n",
        "        r\"\\babgeschlossen(e|es)?(r)? studium\\b\",\n",
        "        r\"\\bausbildung\\b\",\n",
        "        r\"\\bfachinformatiker\\b\",\n",
        "        r\"\\bwirtschaftsinformatik\\b\",\n",
        "        r\"\\babitur\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "EXP_PATTERNS = [\n",
        "    r\"(\\d{1,2})\\s*\\+?\\s*(years?|yrs?)\\b\",\n",
        "    r\"mind\\.?\\s*(\\d{1,2})\\s*jahre\\b\",\n",
        "    r\"über\\s*(\\d{1,2})\\s*j(ahre)?\\b\",\n",
        "    r\"(\\d{1,2})\\s*j(ahre)?\\s*erfahrung\\b\",\n",
        "]\n",
        "\n",
        "WORKLOAD_PATTERNS = [\n",
        "    r\"\\b(20|25|30|32|35|38|40)\\s*(std\\.?|stunden)\\b\",\n",
        "    r\"\\b(20-?30|20\\s*-\\s*30)\\s*stunden\\b\",\n",
        "    r\"\\b70\\s*[-–]\\s*80 ?%\\b\",\n",
        "    r\"\\bteilzeit\\b\",\n",
        "    r\"\\bvollzeit\\b\",\n",
        "    r\"\\bpart[- ]time\\b\",\n",
        "    r\"\\bfull[- ]time\\b\",\n",
        "    r\"\\bhybrid\\b\",\n",
        "    r\"\\bremote\\b\",\n",
        "    r\"\\bon[- ]site\\b\",\n",
        "    r\"\\b3-4\\s*tage\\b\",\n",
        "]\n",
        "\n",
        "LOCATION_PATTERNS = [\n",
        "    r\"\\bberlin\\b\",\n",
        "    r\"\\bmünchen\\b\",\n",
        "    r\"\\bmuenchen\\b\",\n",
        "    r\"\\bfrankfurt\\b\",\n",
        "    r\"\\bhamburg\\b\",\n",
        "    r\"\\bköln\\b\",\n",
        "    r\"\\bkoeln\\b\",\n",
        "    r\"\\bd(ü|u)sseldorf\\b\",\n",
        "    r\"\\bessen\\b\",\n",
        "    r\"\\bstuttgart\\b\",\n",
        "    r\"\\bnürnberg\\b\",\n",
        "    r\"\\bmunich\\b\",\n",
        "    r\"\\bgermany\\b\",\n",
        "    r\"\\bdeutschland\\b\",\n",
        "    r\"\\blocation\\s*:\\s*([^\\n\\r]+)\",\n",
        "]\n",
        "\n",
        "INDUSTRY_PATTERNS = {\n",
        "    \"finance/consulting\": [\n",
        "        r\"\\bfinancial services?\\b\",\n",
        "        r\"\\bprivate equity\\b\",\n",
        "        r\"\\bconsult(ing|ancy)\\b\",\n",
        "        r\"\\binvestment bank\\b\",\n",
        "        r\"\\basset management\\b\",\n",
        "    ],\n",
        "    \"saas/tech\": [\n",
        "        r\"\\bsaas\\b\",\n",
        "        r\"\\b(start[- ]?up|scale[- ]?up)\\b\",\n",
        "        r\"\\bsoftware\\s*company\\b\",\n",
        "        r\"\\btech company\\b\",\n",
        "    ],\n",
        "    \"retail/e-commerce\": [\n",
        "        r\"\\b(retail|e-?commerce)\\b\",\n",
        "        r\"\\bonline shop\\b\",\n",
        "        r\"\\bomnichannel\\b\",\n",
        "    ],\n",
        "    \"energy/green\": [\n",
        "        r\"\\bsolar\\b\",\n",
        "        r\"\\b(battery|batteries)\\b\",\n",
        "        r\"\\b(e-)?mobility\\b\",\n",
        "        r\"\\brenewable\\b\",\n",
        "    ],\n",
        "    \"healthcare/pharma\": [\n",
        "        r\"\\bpharma(ceutical)?\\b\",\n",
        "        r\"\\bmedical devices?\\b\",\n",
        "        r\"\\bhealthcare\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# -------- utility functions (same structure, just reused) --------\n",
        "\n",
        "def _norm(s: str) -> str:\n",
        "    s = re.sub(r\"[ \\t]+\", \" \", str(s))\n",
        "    s = re.sub(r\"\\s+\\n\", \"\\n\", s)\n",
        "    s = re.sub(r\"\\n\\s+\", \"\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def _found(pats, text):\n",
        "    return any(re.search(p, text, flags=re.IGNORECASE) for p in pats)\n",
        "\n",
        "def _collect(dic, text):\n",
        "    seen, out = set(), []\n",
        "    for k, pats in dic.items():\n",
        "        if _found(pats, text) and k not in seen:\n",
        "            seen.add(k)\n",
        "            out.append(k)\n",
        "    return out\n",
        "\n",
        "def _langs(text):\n",
        "    res = []\n",
        "    for code, pats in LANG_PATTERNS.items():\n",
        "        if _found(pats, text):\n",
        "            res.append(code)\n",
        "    # deduplicate preserving order\n",
        "    return list(dict.fromkeys(res))\n",
        "\n",
        "def _degrees(text):\n",
        "    hits = []\n",
        "    for name, pats in DEGREE_MAP.items():\n",
        "        if _found(pats, text):\n",
        "            hits.append(name)\n",
        "    return list(dict.fromkeys(hits))\n",
        "\n",
        "def _years(text):\n",
        "    for p in EXP_PATTERNS:\n",
        "        m = re.search(p, text, flags=re.IGNORECASE)\n",
        "        if m:\n",
        "            try:\n",
        "                return int(m.group(1))\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "def _list_by_pats(pats, text):\n",
        "    vals, seen = [], set()\n",
        "    for p in pats:\n",
        "        m = re.search(p, text, re.IGNORECASE)\n",
        "        if m:\n",
        "            hit = m.group(0)\n",
        "            if hit not in seen:\n",
        "                seen.add(hit)\n",
        "                vals.append(hit)\n",
        "    return vals\n",
        "\n",
        "def _locations_from_text(text):\n",
        "    vals = []\n",
        "    for p in LOCATION_PATTERNS:\n",
        "        m = re.search(p, text, re.IGNORECASE)\n",
        "        if m:\n",
        "            vals.append(m.group(1) if m.lastindex else m.group(0))\n",
        "    vals = [re.sub(r\"^(location\\s*:\\s*)\", \"\", v, flags=re.IGNORECASE).strip(\" :\") for v in vals]\n",
        "    vals = [v.title() for v in vals]\n",
        "    seen, out = set(), []\n",
        "    for v in vals:\n",
        "        if v and v not in seen:\n",
        "            seen.add(v)\n",
        "            out.append(v)\n",
        "    return out\n",
        "\n",
        "def _industry(text):\n",
        "    for name, pats in INDUSTRY_PATTERNS.items():\n",
        "        if _found(pats, text):\n",
        "            return name\n",
        "    return \"\"\n",
        "\n",
        "# --- Description sections classification (still useful for analytics JDs) ---\n",
        "SECTION_HINTS = {\n",
        "    \"responsibilities\": [\n",
        "        r\"\\b(your|deine|ihre)\\s+(tasks|aufgaben|mission)\\b\",\n",
        "        r\"\\bkey responsibilities\\b\",\n",
        "        r\"\\bwhat you will do\\b\",\n",
        "    ],\n",
        "    \"requirements\": [\n",
        "        r\"\\bwhat you bring\\b\",\n",
        "        r\"\\brequirements?\\b\",\n",
        "        r\"\\byour profile\\b\",\n",
        "        r\"\\b(dein|ihr)e(s)? profil\\b\",\n",
        "        r\"\\bqualifikationen?\\b\",\n",
        "        r\"\\bqualifications?\\b\",\n",
        "        r\"\\bskills?\\b\",\n",
        "    ],\n",
        "    \"benefits\": [\n",
        "        r\"\\bbenefits?\\b\",\n",
        "        r\"\\bwhat'?s on offer\\b\",\n",
        "        r\"\\bwhy (us|verimi|enpal|tipico)\\b\",\n",
        "        r\"\\bwir bieten\\b\",\n",
        "        r\"\\bdas erwartet dich\\b\",\n",
        "    ],\n",
        "    \"about_company\": [\n",
        "        r\"\\babout (us|the firm|company)\\b\",\n",
        "        r\"\\bwer wir sind\\b\",\n",
        "        r\"\\bunternehmensbeschreibung\\b\",\n",
        "        r\"\\büber (uns|verimi|enpal|mcmakler)\\b\",\n",
        "        r\"\\bwho are we\\?\\b\",\n",
        "        r\"\\bshort facts\\b\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "def _classify_lines(text):\n",
        "    t = text.replace(\"\\r\", \"\\n\")\n",
        "    # split on newlines, bullets, numbered lists\n",
        "    chunks = re.split(r\"\\n|(?:^|\\n)\\s*[•*–-]\\s*|(?:^|\\n)\\s*\\d+\\.\\s*\", t)\n",
        "    lines = [_norm(x) for x in chunks if _norm(x)]\n",
        "    current = \"other\"\n",
        "    buckets = {\"about_company\": [], \"responsibilities\": [], \"requirements\": [], \"benefits\": [], \"other\": []}\n",
        "    for ln in lines:\n",
        "        lowered = ln.lower()\n",
        "        switched = False\n",
        "        for sec, pats in SECTION_HINTS.items():\n",
        "            if any(re.search(p, lowered) for p in pats):\n",
        "                current = sec\n",
        "                switched = True\n",
        "                break\n",
        "        if switched:\n",
        "            continue\n",
        "        buckets[current].append(ln)\n",
        "    for k in buckets:\n",
        "        buckets[k] = [x for x in buckets[k] if x]\n",
        "    return buckets\n",
        "\n",
        "def parse_description(text: str) -> dict:\n",
        "    t = _norm(text)\n",
        "    sections = _classify_lines(t)\n",
        "    return {\n",
        "        \"hard_skills\": _collect(HARD_SKILLS, t),\n",
        "        \"soft_skills\": _collect(SOFT_SKILLS, t),\n",
        "        \"years_experience\": _years(t),\n",
        "        \"degrees\": _degrees(t),\n",
        "        \"languages\": _langs(t),\n",
        "        \"workload_mentions\": _list_by_pats(WORKLOAD_PATTERNS, t),\n",
        "        \"locations_in_text\": _locations_from_text(t),\n",
        "        \"industry\": _industry(t),\n",
        "        **{f\"{k}_lines\": v for k, v in sections.items()},\n",
        "        \"preview\": t[:300],\n",
        "    }\n",
        "\n",
        "# ---- Apply to cleaned dfc ----\n",
        "parsed = dfc[DESC_COL].fillna(\"\").apply(parse_description)\n",
        "\n",
        "# ---- Flatten parsed dicts into columns ----\n",
        "skills_df = parsed.apply(\n",
        "    lambda x: pd.Series(\n",
        "        {\n",
        "            \"hard_skills\": json.dumps(x[\"hard_skills\"], ensure_ascii=False),\n",
        "            \"soft_skills\": json.dumps(x[\"soft_skills\"], ensure_ascii=False),\n",
        "            \"years_experience\": x[\"years_experience\"],\n",
        "            \"degrees\": json.dumps(x[\"degrees\"], ensure_ascii=False),\n",
        "            \"languages\": json.dumps(x[\"languages\"], ensure_ascii=False),\n",
        "            \"workload_mentions\": json.dumps(x[\"workload_mentions\"], ensure_ascii=False),\n",
        "            \"locations_in_text\": json.dumps(x[\"locations_in_text\"], ensure_ascii=False),\n",
        "            \"industry\": x[\"industry\"],\n",
        "            \"about_company_lines\": json.dumps(x.get(\"about_company_lines\", []), ensure_ascii=False),\n",
        "            \"responsibilities_lines\": json.dumps(x.get(\"responsibilities_lines\", []), ensure_ascii=False),\n",
        "            \"requirements_lines\": json.dumps(x.get(\"requirements_lines\", []), ensure_ascii=False),\n",
        "            \"benefits_lines\": json.dumps(x.get(\"benefits_lines\", []), ensure_ascii=False),\n",
        "            \"other_lines\": json.dumps(x.get(\"other_lines\", []), ensure_ascii=False),\n",
        "            \"preview\": x[\"preview\"],\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "# ---- Merge with dfc ----\n",
        "enriched = pd.concat([dfc.reset_index(drop=True), skills_df.reset_index(drop=True)], axis=1)\n",
        "\n",
        "# ---- Pretty Excel export (no JSON brackets in cells) ----\n",
        "def _to_list(val):\n",
        "    if isinstance(val, list):\n",
        "        return val\n",
        "    if isinstance(val, str):\n",
        "        s = val.strip()\n",
        "        if s == \"\" or s.lower() in {\"nan\", \"none\", \"null\", \"na\", \"n/a\"}:\n",
        "            return []\n",
        "        try:\n",
        "            j = json.loads(s)\n",
        "            if isinstance(j, list):\n",
        "                return j\n",
        "        except Exception:\n",
        "            # fallback: split by common separators\n",
        "            return [p.strip() for p in re.split(r\"[;,\\n]\\s*\", s) if p.strip()]\n",
        "    return []\n",
        "\n",
        "def pretty_list(val):\n",
        "    return \"; \".join(_to_list(val))\n",
        "\n",
        "def pretty_bullets(val):\n",
        "    items = _to_list(val)\n",
        "    return \"\" if not items else \"• \" + \"\\n• \".join(items)\n",
        "\n",
        "pretty = enriched.copy()\n",
        "\n",
        "for c in [\"hard_skills\", \"soft_skills\", \"languages\", \"degrees\", \"workload_mentions\", \"locations_in_text\"]:\n",
        "    if c in pretty.columns:\n",
        "        pretty[c] = pretty[c].apply(pretty_list)\n",
        "\n",
        "for c in [\n",
        "    \"about_company_lines\",\n",
        "    \"responsibilities_lines\",\n",
        "    \"requirements_lines\",\n",
        "    \"benefits_lines\",\n",
        "    \"other_lines\",\n",
        "    \"preview\",\n",
        "    DESC_COL,\n",
        "]:\n",
        "    if c in pretty.columns:\n",
        "        # keep preview/description as plain text, others → bullets\n",
        "        if c in [\"preview\", DESC_COL]:\n",
        "            continue\n",
        "        pretty[c] = pretty[c].apply(pretty_bullets)\n",
        "\n",
        "out_pretty = \"analytics_skills_pretty.xlsx\"\n",
        "\n",
        "with pd.ExcelWriter(out_pretty, engine=\"xlsxwriter\") as writer:\n",
        "    pretty.to_excel(writer, index=False, sheet_name=\"data\")\n",
        "    wb, ws = writer.book, writer.sheets[\"data\"]\n",
        "    wrap = wb.add_format({\"text_wrap\": True, \"valign\": \"top\"})\n",
        "    # Base width\n",
        "    ws.set_column(0, pretty.shape[1] - 1, 18)\n",
        "    # Long text columns — wider with wrap\n",
        "    for col_name in [\n",
        "        c\n",
        "        for c in [\"description_clean\", \"description\", \"preview\",\n",
        "                  \"about_company_lines\", \"responsibilities_lines\",\n",
        "                  \"requirements_lines\", \"benefits_lines\", \"other_lines\"]\n",
        "        if c in pretty.columns\n",
        "    ]:\n",
        "        idx = pretty.columns.get_loc(col_name)\n",
        "        ws.set_column(idx, idx, 45, wrap)\n",
        "    # Skill lists — medium width with wrap\n",
        "    for col_name in [\n",
        "        c\n",
        "        for c in [\"hard_skills\", \"soft_skills\", \"languages\", \"degrees\",\n",
        "                  \"workload_mentions\", \"locations_in_text\"]\n",
        "        if c in pretty.columns\n",
        "    ]:\n",
        "        idx = pretty.columns.get_loc(col_name)\n",
        "        ws.set_column(idx, idx, 28, wrap)"
      ],
      "metadata": {
        "id": "Qlr52NAcNoJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------\n",
        "# Final salary cleaning: convert string → float (EUR, yearly)\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "for col in [\"salary_min (€)\", \"salary_max (€)\"]:\n",
        "    if col in dfc.columns:\n",
        "        dfc[col] = (\n",
        "            dfc[col]\n",
        "            .astype(str)\n",
        "            .str.replace(\"€\", \"\", regex=False)          # remove euro symbol\n",
        "            .str.replace(r\"[^\\d.,-]\", \"\", regex=True)   # keep only digits, dots, commas and dashes\n",
        "            .str.replace(\",\", \".\", regex=False)         # convert comma → dot\n",
        "            .replace(\"\", np.nan)                        # empty strings → NaN\n",
        "            .astype(float)                              # safe conversion to float\n",
        "        )\n",
        "\n",
        "# Rename columns to clean SQL-friendly names\n",
        "dfc = dfc.rename(columns={\n",
        "    \"salary_min (€)\": \"salary_min\",\n",
        "    \"salary_max (€)\": \"salary_max\"\n",
        "})"
      ],
      "metadata": {
        "id": "53g0NEHwN7eV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Find description column\n",
        "desc_col = next(\n",
        "    (c for c in [\"description_clean\", \"description\", \"descriptions\", \"job_description\", \"desc\"]\n",
        "     if c in dfc.columns),\n",
        "    None\n",
        ")\n",
        "if desc_col is None:\n",
        "    raise ValueError(\"No description column found. Expected 'description_clean' or 'description'.\")\n",
        "\n",
        "# 2) Patterns ONLY for Data Analyst / BI / Business Analyst tools\n",
        "PROGRAM_PATTERNS = [\n",
        "    # BI tools\n",
        "    (r\"\\bpower\\s*bi\\b\", \"Power BI\"),\n",
        "    (r\"\\btableau\\b\", \"Tableau\"),\n",
        "    (r\"\\blooker\\b|\\blooker\\s*studio\\b\", \"Looker\"),\n",
        "    (r\"\\bqlik(view|sense)?\\b\", \"Qlik\"),\n",
        "\n",
        "    # SQL + Databases\n",
        "    (r\"\\bsql\\b\", \"SQL\"),\n",
        "    (r\"\\bmysql\\b\", \"MySQL\"),\n",
        "    (r\"\\bpostgres(?:ql)?\\b\", \"PostgreSQL\"),\n",
        "    (r\"\\bsql\\s*server\\b|\\bms\\s*sql\\b\", \"SQL Server\"),\n",
        "    (r\"\\boracle\\b\", \"Oracle DB\"),\n",
        "    (r\"\\bsnowflake\\b\", \"Snowflake\"),\n",
        "    (r\"\\bbigquery\\b\", \"BigQuery\"),\n",
        "    (r\"\\bredshift\\b\", \"Redshift\"),\n",
        "    (r\"\\bsqlite\\b\", \"SQLite\"),\n",
        "\n",
        "    # Python / R\n",
        "    (r\"\\bpython\\b\", \"Python\"),\n",
        "    (r\"\\bpandas\\b\", \"Pandas\"),\n",
        "    (r\"\\bnumpy\\b\", \"NumPy\"),\n",
        "    (r\"\\bscikit[-\\s]*learn\\b\", \"Scikit-Learn\"),\n",
        "    (r\"\\bpyspark\\b\", \"PySpark\"),\n",
        "    (r\"\\br\\b\", \"R\"),\n",
        "\n",
        "    # Cloud\n",
        "    (r\"\\baws\\b|\\bamazon\\s*web\\s*services\\b\", \"AWS\"),\n",
        "    (r\"\\bazure\\b\", \"Azure\"),\n",
        "    (r\"\\bgcp\\b|\\bgoogle\\s*cloud\\b\", \"GCP\"),\n",
        "    (r\"\\bazure\\s*data\\s*factory\\b\", \"Azure Data Factory\"),\n",
        "    (r\"\\bazure\\s*synapse\\b\", \"Azure Synapse\"),\n",
        "    (r\"\\bdatabricks\\b\", \"Databricks\"),\n",
        "\n",
        "    # ETL / Orchestration\n",
        "    (r\"\\bairflow\\b\", \"Airflow\"),\n",
        "    (r\"\\betl\\b\", \"ETL\"),\n",
        "    (r\"\\belt\\b\", \"ELT\"),\n",
        "    (r\"\\bfivetran\\b\", \"Fivetran\"),\n",
        "    (r\"\\bmatillion\\b\", \"Matillion\"),\n",
        "    (r\"\\binformatica\\b\", \"Informatica\"),\n",
        "    (r\"\\btalend\\b\", \"Talend\"),\n",
        "    (r\"\\bdbt\\b\", \"dbt\"),\n",
        "\n",
        "    # Version control\n",
        "    (r\"\\bgit\\b\", \"Git\"),\n",
        "    (r\"\\bgithub\\b\", \"GitHub\"),\n",
        "    (r\"\\bgitlab\\b\", \"GitLab\"),\n",
        "\n",
        "    # Spreadsheets\n",
        "    (r\"\\bexcel\\b\", \"Excel\"),\n",
        "    (r\"\\bgoogle\\s*sheets\\b\", \"Google Sheets\")\n",
        "]\n",
        "\n",
        "\n",
        "def extract_programs_from_text(text: str):\n",
        "    \"\"\"Extract analytics tools from the description text.\"\"\"\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return []\n",
        "    t = text.lower()\n",
        "    found = []\n",
        "\n",
        "    for pat, label in PROGRAM_PATTERNS:\n",
        "        if re.search(pat, t, flags=re.IGNORECASE):\n",
        "            found.append(label)\n",
        "\n",
        "    # unique, preserve order\n",
        "    seen, uniq = set(), []\n",
        "    for x in found:\n",
        "        if x not in seen:\n",
        "            seen.add(x)\n",
        "            uniq.append(x)\n",
        "    return uniq\n",
        "\n",
        "# 3) Create Programs column\n",
        "dfc[\"Programs\"] = dfc[desc_col].fillna(\"\").apply(extract_programs_from_text)\n",
        "\n",
        "# 4) Explode to one tool per row\n",
        "dfc = dfc.explode(\"Programs\", ignore_index=True)\n",
        "\n",
        "# 5) Drop empty values\n",
        "dfc = dfc[dfc[\"Programs\"].notna() & (dfc[\"Programs\"].str.strip() != \"\")]\n",
        "\n",
        "print(f\"Строк после взрыва Programs: {len(dfc)}\")\n",
        "print(dfc[\"Programs\"].value_counts().head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1_1YiOpN9Z6",
        "outputId": "495edee2-7469-4f42-c198-1f5720c4c60f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Строк после взрыва Programs: 6526\n",
            "Programs\n",
            "SQL          1016\n",
            "Python        818\n",
            "Tableau       764\n",
            "Power BI      708\n",
            "R             459\n",
            "Excel         372\n",
            "Looker        245\n",
            "Snowflake     232\n",
            "Qlik          210\n",
            "ETL           206\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Source file exported from previous step (analytics_skills_pretty.xlsx)\n",
        "src_file = \"analytics_skills_pretty.xlsx\"\n",
        "df = pd.read_excel(src_file)\n",
        "\n",
        "# List of columns that contain lists (either as ';' separated strings or JSON)\n",
        "list_cols = [\n",
        "    \"hard_skills\", \"soft_skills\", \"languages\", \"degrees\",\n",
        "    \"workload_mentions\", \"locations_in_text\",\n",
        "    \"about_company_lines\", \"responsibilities_lines\",\n",
        "    \"requirements_lines\", \"benefits_lines\", \"other_lines\"\n",
        "]\n",
        "\n",
        "rows = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    # Convert each \"list-like\" column into a proper Python list\n",
        "    expanded_lists = {}\n",
        "    for col in list_cols:\n",
        "        val = row.get(col, \"\")\n",
        "        if pd.isna(val) or str(val).strip() == \"\":\n",
        "            expanded_lists[col] = []\n",
        "        else:\n",
        "            # If it's JSON (starts with '[') → parse JSON\n",
        "            # Otherwise → split by ';'\n",
        "            try:\n",
        "                if isinstance(val, str) and val.strip().startswith(\"[\"):\n",
        "                    expanded_lists[col] = json.loads(val)\n",
        "                else:\n",
        "                    expanded_lists[col] = [\n",
        "                        v.strip() for v in str(val).split(\";\") if v.strip()\n",
        "                    ]\n",
        "            except Exception:\n",
        "                expanded_lists[col] = [\n",
        "                    v.strip() for v in str(val).split(\";\") if v.strip()\n",
        "                ]\n",
        "\n",
        "    # Determine max length of all lists for this row\n",
        "    max_len = max(len(expanded_lists[c]) for c in list_cols)\n",
        "\n",
        "    # Create max_len rows, duplicating scalar fields\n",
        "    for i in range(max_len if max_len > 0 else 1):\n",
        "        new_row = row.to_dict()\n",
        "        for col in list_cols:\n",
        "            items = expanded_lists[col]\n",
        "            new_row[col] = items[i] if i < len(items) else None\n",
        "        rows.append(new_row)\n",
        "\n",
        "df_exploded = pd.DataFrame(rows)\n",
        "\n",
        "out_file = \"analytics_jobs_exploded_2025-11-19.xlsx\"\n",
        "df_exploded.to_excel(out_file, index=False)\n",
        "\n",
        "print(f\"Exploded file: {out_file}, rows: {len(df_exploded)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPekXVe4OmjV",
        "outputId": "323013ea-f48f-43d7-9ce5-b2786efe7bd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploded file: analytics_jobs_exploded_2025-11-19.xlsx, rows: 5896\n"
          ]
        }
      ]
    }
  ]
}